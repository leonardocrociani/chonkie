---
title: 'SDPM Chunker'
description: 'Split text using Semantic Double-Pass Merging for improved context preservation'
icon: 'layer-group'
---

The `SDPMChunker` extends semantic chunking by using a double-pass merging approach. It first groups content by semantic similarity, then merges similar groups within a skip window, allowing it to connect related content that may not be consecutive in the text. This technique is particularly useful for documents with recurring themes or concepts spread apart.

## API Reference
To use the `SDPMChunker` via the API, check out the [API reference documentation](../../api-reference/sdpm-chunker).

## Installation

SDPMChunker requires additional dependencies for semantic capabilities. You can install it with:

```bash
pip install "chonkie[semantic]"
```

<Info>For installation instructions, see the [Installation Guide](/getting-started/installation).</Info>

## Initialization

```python
from chonkie import SDPMChunker

# Basic initialization with default parameters
chunker = SDPMChunker(
    embedding_model="minishlab/potion-base-8M",  # Default model
    threshold=0.5,                              # Similarity threshold (0-1)
    chunk_size=512,                             # Maximum tokens per chunk
    min_sentences=1,                            # Initial sentences per chunk
    skip_window=1                               # Number of chunks to skip when looking for similarities
)
```

## Parameters

<ParamField
    path="embedding_model"
    type="Union[str, BaseEmbeddings]"
    default="minishlab/potion-base-8M"
>
    Model identifier or embedding model instance
</ParamField>

<ParamField
    path="chunk_size"
    type="int"
    default="512"
>
    Maximum tokens per chunk
</ParamField>

<ParamField
    path="mode"
    type="str"
    default="window"
>
    Mode for grouping sentences, either "cumulative" or "window"
</ParamField>

<ParamField
    path="threshold"
    type="Union[float, int, str]"
    default="auto"
>
    When in the range [0,1], denotes the similarity threshold to consider sentences similar.
    When in the range (1,100], interprets the given value as a percentile threshold.
    When set to "auto", the threshold is automatically calculated.
</ParamField>

<ParamField
    path="similarity_window"
    type="int"
    default="1"
>
    Number of sentences to consider for similarity threshold calculation
</ParamField>

<ParamField
    path="min_sentences"
    type="int"
    default="1"
>
    Minimum number of sentences per chunk
</ParamField>

<ParamField
    path="min_chunk_size"
    type="int"
    default="2"
>
    Minimum tokens per chunk
</ParamField>

<ParamField
    path="min_characters_per_sentence"
    type="int"
    default="12"
>
    Minimum number of characters per sentence
</ParamField>

<ParamField
    path="threshold_step"
    type="float"
    default="0.01"
>
    Step size for threshold calculation
</ParamField>

<ParamField
    path="delim"
    type="Union[str, List[str]]"
    default="['.', '!', '?', '\n']"
>
    Delimiters to split sentences on
</ParamField>

<ParamField
    path="include_delim"
    type='Optional[Literal["prev", "next"]]'
    default="prev"
>
    Include delimiters in the chunk text. If so, specify whether to include the previous or next delimiter.
</ParamField>

<ParamField
    path="skip_window"
    type="int"
    default="1"
>
    Number of chunks to skip when looking for similarities
</ParamField>

<ParamField
    path="return_type"
    type="Literal['chunks', 'texts']"
    default="chunks"
>
    Return type for the chunking process. If "chunks", returns a list of `SemanticChunk` objects. If "texts", returns a list of strings.
</ParamField>

## Usage

### Single Text Chunking

```python
text = """The neural network processes input data through layers.
Training data is essential for model performance.
GPUs accelerate neural network computations significantly.
Quality training data improves model accuracy.
TPUs provide specialized hardware for deep learning.
Data preprocessing is a crucial step in training."""

chunks = chunker.chunk(text)

for chunk in chunks:
    print(f"Chunk text: {chunk.text}")
    print(f"Token count: {chunk.token_count}")
    print(f"Number of sentences: {len(chunk.sentences)}")
```

### Batch Chunking

```python
texts = [
    "Document with scattered but related content...",
    "Another document with similar patterns..."
]
batch_chunks = chunker.chunk_batch(texts)

for doc_chunks in batch_chunks:
    for chunk in doc_chunks:
        print(f"Chunk: {chunk.text}")
```

## Supported Embeddings

SDPMChunker supports multiple embedding providers through Chonkie's embedding system. 
See the [Embeddings Overview](/embeddings/overview) for more information.

## Return Type

SDPMChunker returns `SemanticChunk` objects with optimized storage using slots:

```python
@dataclass
class SemanticSentence(Sentence):
    text: str
    start_index: int
    end_index: int
    token_count: int
    embedding: Optional[np.ndarray]  # Sentence embedding vector
    
    __slots__ = ['embedding']  # Optimized memory usage

@dataclass
class SemanticChunk(SentenceChunk):
    text: str
    start_index: int
    end_index: int
    token_count: int
    sentences: List[SemanticSentence]
```
